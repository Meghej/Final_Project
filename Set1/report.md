**Result** : We obtained a RMSE of  value 2.3 using Gradient Boost Regressor.   

**Reason For Choosing GBR** : It provides us with high accuracy also since it can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible. Now since we had a lot of features in this problem, the tunining for individual parameters wasn't possible hence this was the best alternative we had. 
Please also note that In my system, other package installation got failed and I wasn't able to apply other models otherwise the best solution would have been to take a collective weighted average of other models and then get highly reduced error.   

**Problems faced** : Same as that of Titanic ( Conversion of Categorical Data into Numerical Data, Filling the missing data. I tackled them using the conversion by TRANSFORMATION using sklearn tools. Also to tackle missing datas, whereever the missing data was less it was intuitive to remove those entries and also replaced few datas using mean values from the dataset. For age the better solution was to replace it with median.) But the most difficult part was that there was a lot of features available to be analysed. This took a lot of time and also making the decision what to do for above problem for each of them was also a difficult task. The grouping method based on related features was helpful, thanks to the database description at kaggle.   

**Conclusion** : we followed the data science process by first obtaining the data, then cleaning and pre-processing the data, then exploring the data and building, evaluating the results and communicating them with visualizations. Since the RMSE obtained is quite large, it is advised that the knowledge of actual price would be better and also the RMSE can be heavily reduced by using multiple models rather than only 1. 
